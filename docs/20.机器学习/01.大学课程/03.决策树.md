---
title: 决策树
date: 2021-09-20 16:00:00
permalink: /pages/B6g5DQ
categories: 
  - 机器学习
tags: 
  - false
author: 
  name: yangxin
  link: https://github.com/yangxin6/MechineLearning/tree/master/lesson3
---



例：下面为8位同学的数据

![dt](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/dt.31c44kkiag60.png)

根据头发和声音进行分类：

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/dt1.74zw2ipo0nc0.png" alt="dt1" style="zoom:70%;" />

## 概念

- 决策树（Decision Tree）是一个树结构（可以是二叉树或者非二叉树）。

- 决策树分为：
  - **分类树**：对<mark>离散</mark>变量做决策树
  - **回归树**：对<mark>连续</mark>变量做决策树

**非叶节点**表示一个特征树形上的测试，**分支**代表这个特征属性在某个值遇上的输出，**叶节点**存放在一个类别。



决策树是 一中归纳学习的方法，从假设空间中搜索一个拟合训练样例的假设。



- **特征选择**：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的**分裂标准**，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。

- **决策树生成**：根据选择的特征评估标准，从上至下**递归**地生成子节点，直到数据集不可分则停止决策树生长。对树结构来说，递归结构是最容易理解的方式。
- **决策树的剪枝**：决策树容易过拟合，一般来需要剪枝，缩小树结构规则，**缓解过拟合**，剪枝技术有预剪枝和后剪枝两种。



## 信息熵

- 在信息论与概率论中，熵（entropy）用于表示“随机变量不确定性的度量”
  - $H(X)=-\sum \limits^n_{i=1}p(x_i)log(p(x_i))$

- $X$ 代表样本总量，$n$代表类别数，$p(x_i)$代表类别 $x_i$ 的概率
- 假设样本集合D有K个类别，每个类别的概率是$|C_k|/|D|$，其中$|C_k|$表示类别 $k$ 的样本个数，$|D|$表示样本总数，则对于样本集合 $D$ 来说熵（经验熵）为：
  - $H(D)=-\sum \limits^N_{k-1} \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}$



## 条件熵

$H(D|A)=\sum \limits^n_{i=0}H(D|A=x_i)=\sum \limits^n_{i=0}p(x_i)H(D_{x_i})$

## 信息增益

信息增益是相对于特征而言的。特征A对训练数据集$D$的信息增益 $g(D,A)$，定义为集合D的信息熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即

$g(D,A)=H(D)-H(D|A)$

$g(D,A)=H(D)-\sum \limits_{v\in Values(A)} \frac{|D_v|}{|D|}H(D_v)$



**例子**

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/dk2.1uym0x4y7beo.png" alt="dk2" style="zoom:90%;" />

在15个数据中，结果分类为2个，放贷或不放贷，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集$X$ 的信息熵为： 

$H(X)=-\frac9{15}log_2\frac9{15}-\frac6{15}log_2\frac6{15}=0.971$



- 年龄特征 $A_1$ （青年）的**信息增益**

  - $g(D,A_1)=H(D)-[\frac5{15}H(D_1)+\frac5{15}H(D_2)+\frac5{15}H(D_3)]$

    ​				$= 0.971-[\frac5{15}(-\frac25log_2\frac25-\frac35log_2\frac35)+\frac5{15}(-\frac35log_2\frac35-\frac25log_2\frac25)+\frac5{15}(-\frac45log_2\frac45-\frac15log_2\frac15)]$

    $=0.971-0.888=0.083$

- 计算其余特征的信息增益 $g(D,A_2),g(D,A_3),g(D,A_4)$

  - $g(D,A_2)=H(D)-[\frac5{15}H(D_1)+\frac{10}{15}H(D_2)]$

    ​				$=0.971-[\frac5{15} \times 0 + \frac{10}{15}(-\frac4{10}log_2\frac4{10} -\frac6{10}log_2\frac6{10})]$

    ​				$=0.971-0.647=0.324$

  - $g(D,A_3)=0.420$
  - $g(D,A_3)=0.363$

最后 比较特征的信息增益，$A_3$（有自己的房子） 的信息增益值最大，所以选 $A_3$ 作为最优特征。

使用 $A_3$ 将数据集分为两个子集，在分别计算两个子集的特征信息增益，继续选下去，从而构造出决策树。



## 构造决策树

- 使用统计测试来确定每一个实例属性（特征向量分量）单独分类训练样例的能力。

- 分类能力**最好的属性被选作树的根节**点的测试。

- 为根节点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支之下。

- **重复**整个过程，用每个分支关联的训练样本来选取在该分支下被测试的最佳属性。

- 形成了对合格决策树的贪婪搜索，也就是算法从不回溯重新考虑以前的选择。



具有最高信息增益的属性为分类能力最好的属性。



## 信息增益比

- 信息增益**偏向取值较多**的特征。
  - 当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，所以信息增益比较偏向取值较多的特征。

- **信息增益比**：
  - $g(D,A)=\frac{g(D,A)}{H_A(D)}$
  - $H_A(D)=-\sum \limits^{n}_{i=1} \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$
  - 其中的 $H(D)$，对于样本集合$D$，将当前特征$A$作为随机变量（取值特征$A$的各个特征值），求得的经验熵。
    - 之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合$D$进行划分，计算熵$H(D)$。

- **信息增益比本质**：是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。
- **惩罚参数**：数据集$D$以特征$A$作为随机变量的熵的倒数，即：将特征$A$取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）
  - 惩罚参数=$\frac1{H_A(D)} = \frac1{-\sum^n_{i=1}\frac{|D_i|}{|D|}log_2 \frac{|D_i|}{|D|}}$
- 缺点：信息增益比偏向取值较少的特征
- 原因：当特征取值较少时$H_A(D)$的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。
- 使用信息增益比：基于以上缺点，并不是直接选择信息增益比最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后文些特征中再选择信息增益比最高的特征。

:::tip

- ID3采用信息增益

- C4.5采用信息增益比

- CART算法使用“基尼指数”来选择划分属性，选择基尼值最小的属性作为划分属性.

:::





:::note

深入学习：

- Bagging
- 随机森林(Random Forest)
- Boosting

:::

