---
title: K均值聚类
date: 2021-09-20 22:00:00
permalink: /pages/5IPEsi
categories: 
  - 机器学习
tags: 
  - KMeans
author: 
  name: yangxin
  link: https://github.com/yangxin6/MechineLearning/tree/master/lesson4
---

## 聚类

**聚类**就是对大量未标注数据构成的数据集，按数据的内在相似性将数据集划分为多个类别，使**类别内**的数据**相似度较大**而**类别间**的数据**相似度较小**，聚类属于**无监督**的学习方法。

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/kmeans.5i4iu6z1xe8.png" alt="kmeans" style="zoom:77%;" />

## K均值聚类

- K均值（k-means）聚类的目标是在数据中找到由变量 k 标记的组（簇），基于数据的特征，通过算法迭代，将每个数据点分配给k个组中的一个。基于数据的特征相似性对数据点进行聚类。K均值聚类的结果是：
- K族的质心，用于标记新数据
- 训练数据的标签，每个数据点分配给一个集群



相关概念

- **K值**：要得到的簇（组、集群）的个数
- **质心**：每个簇的均值向量，同组数据向量各维取平均即可
- **距离度量**：常用欧几里得距离和余弦相似度等



## 数学原理

如果用数学表达式表示，假设簇划分为 $(C_1,...C_n)$，则我们聚类的目标是最小化平方误差$E$：

​	$E = \sum \limits^k_{i-1} \sum \limits_{x\in C_i} ||x-u_i||^2_2$

其中 $u_i$ 是簇 $C_i$ 的均值向量，即质心，表达式为：

  $u_i = \frac1{|C_i|}\sum \limits{x\in C_i}x$

直接求E的最小值并不容易，这是一个**NP**难（NP-hard）问题，因此只能采用启发式的迭代方法。



## 算法流程

1. 首先确定一个k值，即我们希望将数据集经过聚类后得到k个集合
2. 从数据集中随机选择k个数据点作为质心
3. 对数据集中每一个点，计算其与每一个质心的距离（如欧式距离），离哪个质心近，就划分到那个质心所属的集合
4. 把所有数据归好集合后，一共有k个集合，然后重新计算每个集合的质心（同组数据向量各维平均）
5. 如果新质心和原质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），可以认为聚类已经达到期望的结果，算法终止
6. 如果新质心和原质心距离变化很大，用新质心更新原质心，并迭代步骤3~5。



## 相似度度量

聚类是根据数据的内在的相似性进行的。数据的内在的相似性主要根**据数据的相似度或者距离**来定义的，比较常见的有：

- 欧氏距离：
  - $dist(X,Y)=(\sum \limits^n_{i=1}|x_i-y_i|^p)^\frac1p$

- 杰卡德相似系数：
  - $J(A,B) = \frac{|A\bigcap B|}{|A\bigcup B|}$
  - 一般度量集合之间的相似性
- 余弦相似度：
  - $\cos \theta = \frac{a^Tb}{|a||b|}$   <img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/cos.4u868dxu7gk0.png" alt="cos" style="zoom:50%;" />

- Pearson 相似系数：
  - <img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/pearson.3pk40yxka0u0.png" alt="pearson" style="zoom:63%;" /> 

- 相对熵（K-L距离）
  - <img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/KL.tpc2d1y06o0.png" alt="KL" style="zoom:50%;" /> 

